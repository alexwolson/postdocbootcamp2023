{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kbu2lg_GcvW6"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/eldanc/mlbootcamp2023/blob/main/lab_3_2_autoencoders.ipynb)\n",
    "\n",
    "\n",
    "# UofT DSI-CARTE Postdoc Bootcamp\n",
    "#### Wednesday, July 19, 2023\n",
    "#### Autoencoders - Lab 1, Day 2\n",
    "#### Teaching team: Teaching team: Alex Olson, Nakul Upadhya, Shehnaz Islam\n",
    "##### Lab author: Alexander Olson, aolson@mie.utoronto.ca, edited by Jake Mosseri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y4zdERQcvW_"
   },
   "source": [
    "Today we are going to learn about a type of deep learning model used for dimensionality reduction and data compression called Autoencoders.\n",
    "\n",
    "An autoencoder is a special type of neural network with an unusual task: for some input X, all it has to do is return that input X as accurately as possible. But there's a catch, of course! Between the input and the output, the number of nodes in each hidden layer actually gets progressively smaller. This means that in the first half of the network, the network must learn how to represent the input in ever more compact formats. The second half does this in reverse, taking the smallest representation of the input and expanding it back out into the full, original data.\n",
    "\n",
    "Thus autoencoders consist of an encoder and a decoder part, which are symmetric in structure. The encoder part compresses the input data into a lower-dimensional representation, while the decoder part tries to reconstruct the original input from this compressed representation.\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/ae.png?raw=1\" alt=\"cross-val\" width=\"500\"/>\n",
    "\n",
    "Autoencoders are much more powerful than traditional dimensionality reduction methods such as PCA and t-SNE when it comes to learning compact representations of data, as we will see here. This is because while PCA finds linear combinations of the original features to create the lower-dimensional representation, autoencoders have the ability to learn non-linear mappings due to the non-linear activation functions after each layer.\n",
    "\n",
    "Now lets first implement PCA and give it an autoencoder's task: first, reduce the dimensionality of a dataset, and then recover the full dimensionality of the original input.\n",
    "\n",
    "We will use MNIST dataset today, through a slightly different mechanism to ease its compatibility with Pytorch. Run the code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqnLjXQAcvXA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2iyKA1wcvXB"
   },
   "outputs": [],
   "source": [
    "# Path parameters\n",
    "MNIST_PATH = Path(\"./mnist/\")\n",
    "DOWNLOAD_MNIST = not MNIST_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-5uEYgzcvXC"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist/',\n",
    "    train=True,                                     # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),    # Converts a PIL.Image or numpy.ndarray to\n",
    "                                                    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,                        # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVv9L5ZYcvXD"
   },
   "outputs": [],
   "source": [
    "print('Training data size:\\t',train_data.data.size())     # (60000, 28, 28)\n",
    "print('Testing data size:\\t',train_data.targets.size())   # (60000)\n",
    "plt.imshow(train_data.data[2].numpy(), cmap='gray')\n",
    "plt.title('%i' % train_data.targets[2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl0EdYfocvXE"
   },
   "outputs": [],
   "source": [
    "# Implementing PCA for autoencoder-like task\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# This code reduces the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(train_data.data.numpy().reshape(60000, 28*28)) # need to reshape to convert a 60000x28x28 vector to 60000x784 because PCA takes 2D array\n",
    "data_pca = pca.transform(train_data.data.numpy().reshape(60000, 28*28))\n",
    "\n",
    "# This code transforms the data back to its original size\n",
    "data_pca_inv = pca.inverse_transform(data_pca)\n",
    "plt.imshow(data_pca_inv[2].reshape(28,28), cmap='gray') # Plot the image in orginal dimension\n",
    "plt.title('%i' % train_data.targets[2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js8De-lkcvXF"
   },
   "source": [
    "**YOUR TURN**\n",
    "* How does the image look after dimensionality reduction compared to the input? ______\n",
    "* Why might it look this way? ______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkvDQ-ngcvXG"
   },
   "source": [
    "Let's now move on to building an autoencoder for the same task. Autoencoders are easy networks to build, split into the _encoder_, which 'steps' the data down to the final compact representation, and the _decoder_, which is a mirror image of the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0b71VvTcvXG"
   },
   "outputs": [],
   "source": [
    "myEncoder = nn.Sequential(\n",
    "    nn.Linear(28*28, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 12),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(12, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDBfdwsAcvXH"
   },
   "source": [
    "As you may expect, designing the structure of the encoder is something of an art, and it requires balance between the time and input data required to train the network, and performance. Here we are using four step-down operations (the linear layers), which take us from the input of size 784 down to just two dimensions at the bottom. Between each step-down layer is a non-linear activation layer.\n",
    "\n",
    "**YOUR TURN**\n",
    "\n",
    "It would of course be possible to go straight from the input size to the final number of dimensions, but we would lose an incredibly important aspect of neural networks in doing so.\n",
    "\n",
    "* What would we miss out on? ______\n",
    "* Why is this a problem? ______\n",
    "\n",
    "In the cell below, build the structure of the decoder layer for our network. Remember, this is a mirror image of our encoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea1Bqw54cvXH"
   },
   "outputs": [],
   "source": [
    "myDecoder = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ge91UswcvXI"
   },
   "source": [
    "Finally, we just need some boilerplate class code to bring the whole thing together into a PyTorch network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyTHANKjcvXI"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = myEncoder\n",
    "        self.decoder = myDecoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOeLISfQcvXI"
   },
   "source": [
    "Now let's create an instance of our network. We also need to define a few other parameters, like the loss function and the optimizer.\n",
    "\n",
    "For the loss function, we will be using Mean Squared Error, which we covered in the second lab. For the optimizer, let's use an advanced optimizer called Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VedL8oiWcvXJ"
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder()\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.005)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlozDJtFcvXJ"
   },
   "source": [
    "We'll use a helper function during training which will pass us the data as we go. It's important to remember that for an autoencoder, the input and the label are identical, so we don't have any labels per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssW1oQUQcvXK"
   },
   "outputs": [],
   "source": [
    "train_loader = Data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3BwMYIfcvXK"
   },
   "source": [
    "Our training function is going to show us the recovered images at the end of each epoch, to help us get an idea of how the training process is going. Beforehand, we will just plot out five of the digits so we can compare our autoencoder's output to what the target looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOWHG7AecvXK"
   },
   "outputs": [],
   "source": [
    "N_TEST_IMG = 5\n",
    "# initialize figure\n",
    "f, a = plt.subplots(1, N_TEST_IMG, figsize=(5, 2))\n",
    "# original data (first row) for viewing\n",
    "view_data = train_data.data[:N_TEST_IMG].view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "for i in range(N_TEST_IMG):\n",
    "    a[i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap='gray'); a[i].set_xticks(()); a[i].set_yticks(())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4w34AZlcvXL"
   },
   "source": [
    "OK, now we're ready to train! Before running this code, make sure you understand what is happening between the dashed lines (the rest is just there to draw the graphs, so it's not important). The comments should help. Once you feel comfortable, run the cell! It will take a little while to complete, but you will get progress updates as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BkZijX_cvXL"
   },
   "outputs": [],
   "source": [
    "for epoch in range(25):\n",
    "    for step, (x, _) in enumerate(train_loader):\n",
    "        #----------------------------------------------------------------------------#\n",
    "        b_x = x.view(-1, 28*28)   # batch x, shape (batch, 28*28)\n",
    "        b_y = x.view(-1, 28*28)   # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = loss_func(decoded, b_y)      # mean square error\n",
    "        optimizer.zero_grad()               # clear gradients for this training step\n",
    "        loss.backward()                     # backpropagation, compute gradients\n",
    "        optimizer.step()                    # apply gradients\n",
    "        #----------------------------------------------------------------------------#\n",
    "    print('Epoch: ', epoch, '| train loss: %.4f' % loss.data.numpy())\n",
    "    f, a = plt.subplots(1, N_TEST_IMG, figsize=(5, 2))\n",
    "\n",
    "    # plotting decoded image (second row)\n",
    "    _, decoded_data = autoencoder(view_data)\n",
    "    for i in range(N_TEST_IMG):\n",
    "        a[i].clear()\n",
    "        a[i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap='gray')\n",
    "        a[i].set_xticks(()); a[i].set_yticks(())\n",
    "    plt.draw(); plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaWVZYPOcvXL"
   },
   "source": [
    "The third image in the column is our Autoencoder's recovered version of the number 4 we looked at with PCA. How does it look? Could it be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYa_8J0zcvXM"
   },
   "source": [
    "We can also plot the encoded data of the autoencoder, to see how it has represented the data in lower 2 dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCIA00n3cvXM"
   },
   "outputs": [],
   "source": [
    "view_data = train_data.data.view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "encoded_data, _ = autoencoder(view_data)\n",
    "X, Y = encoded_data.data[:, 0].numpy(), encoded_data.data[:, 1].numpy() # 2 Dimensions of encoded data\n",
    "values = train_data.targets.numpy()\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.scatter(X, Y,\n",
    "            c=values, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.colormaps.get_cmap('nipy_spectral'))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going to plot different outputs based on changes to the representation (encoded) space. We will loop through all different combinations between -1 and 1 to see what the autoencoder has learned."
   ],
   "metadata": {
    "id": "TZYAWeGlorfX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "f, a = plt.subplots(9, 9, figsize=(12, 12))\n",
    "for i,v in enumerate([1,0.75,0.5,0.25,0,-0.25,-0.5,-0.75,-1]):\n",
    "  for j,k in enumerate([-1,-0.75,-0.5,-0.25,0,0.25,0.5,0.75,1]):\n",
    "    a[i,j].imshow(np.reshape(\n",
    "        autoencoder.decoder(Tensor([k,v])).detach().numpy(),\n",
    "        (28, 28)), cmap='gray'); a[i,j].set_xticks(()); a[i,j].set_yticks(())"
   ],
   "metadata": {
    "id": "XQVOfMGXoKNd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's utilize the autoencoder's learned embedding space to classify digits. We'll convert input images into embedded features using the encoder of the autoencoder, then pass them to a classifier with their corresponding labels for training. You have the flexibility to choose a classifier such as logistic regression or others. Part of the code is provided below.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Implement a classifier of your choice to train on the input embeddings and labels. You can use a simple classifier such as logistic regression classifier or can try other classifiers of your choice. _________\n",
    "* What is the prediction accuracy of the classifier on the test set? _________"
   ],
   "metadata": {
    "collapsed": false,
    "id": "WVofdEA1XtfE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Train set\n",
    "n_samples_train = 100 # choose a small training set\n",
    "train_set= train_data.data[:n_samples].view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "train_labels = train_data.targets[:n_samples].numpy()\n",
    "\n",
    "# Test set\n",
    "n_samples_test= 10 # choose 100 a samples for testing the classifier\n",
    "test_set= train_data.data[n_samples:n_samples+n_samples_test].view(-1, 28*28).type(torch.FloatTensor)/255.\n",
    "test_labels = train_data.targets[n_samples:n_samples+n_samples_test].numpy()\n",
    "\n",
    "# Obtain embedded representations of train set images using the encoder part of the autoencoder\n",
    "embedded_train= autoencoder.encoder(train_set).detach().numpy()"
   ],
   "metadata": {
    "id": "c5dvfk8TXtfE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Train your chosen classifier using embedded features and corresponding labels"
   ],
   "metadata": {
    "id": "m7GtXKPvXtfF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compute the prediction accuracy of the classifier on the test set\n"
   ],
   "metadata": {
    "id": "hw09W9waXtfF"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
