{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kbu2lg_GcvW6"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexwolson/postdocbootcamp2023/blob/main/lab_2_1_autoencoders.ipynb)\n",
    "\n",
    "\n",
    "# UofT DSI-CARTE Postdoc Bootcamp\n",
    "#### Wednesday, July 19, 2023\n",
    "#### Autoencoders - Lab 1, Day 2\n",
    "#### Teaching team: Teaching team: Alex Olson, Nakul Upadhya, Shehnaz Islam\n",
    "##### Lab author: Alex Olson, edited by Jake Mosseri and Shehnaz Islam\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Y4zdERQcvW_"
   },
   "source": [
    "Today we are going to learn about a type of deep learning model used for dimensionality reduction and data compression called Autoencoders.\n",
    "\n",
    "An autoencoder is a special type of neural network with an unusual task: for some input X, all it has to do is return that input X as accurately as possible. But there's a catch, of course! Between the input and the output, the number of nodes in each hidden layer actually gets progressively smaller. This means that in the first half of the network, the network must learn how to represent the input in ever more compact formats. The second half does this in reverse, taking the smallest representation of the input and expanding it back out into the full, original data.\n",
    "\n",
    "Thus, autoencoders consist of an encoder and a decoder part, which are symmetric in structure. The encoder part compresses the input data into a lower-dimensional representation, while the decoder part tries to reconstruct the original input from this compressed representation.\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/ae.png?raw=1\" alt=\"cross-val\" width=\"500\"/>\n",
    "\n",
    "Autoencoders are much more powerful than traditional dimensionality reduction methods such as PCA and t-SNE when it comes to learning compact representations of data, as we will see here. This is because while PCA finds linear combinations of the original features to create the lower-dimensional representation, autoencoders have the ability to learn non-linear mappings due to the non-linear activation functions after each layer.\n",
    "\n",
    "Now lets first implement PCA and give it an autoencoder's task: first, reduce the dimensionality of a dataset, and then recover the full dimensionality of the original input.\n",
    "\n",
    "We will use MNIST dataset today, through a slightly different mechanism to ease its compatibility with Pytorch. Run the code below to download the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FqnLjXQAcvXA"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2iyKA1wcvXB"
   },
   "outputs": [],
   "source": [
    "# Path parameters\n",
    "MNIST_PATH = Path(\"./mnist/\")\n",
    "DOWNLOAD_MNIST = not MNIST_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6-5uEYgzcvXC"
   },
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./mnist/\",\n",
    "    train=True,  # this is training data\n",
    "    transform=torchvision.transforms.ToTensor(),  # Converts a PIL.Image or numpy.ndarray to\n",
    "    # torch.FloatTensor of shape (C x H x W) and normalize in the range [0.0, 1.0]\n",
    "    download=DOWNLOAD_MNIST,  # download it if you don't have it\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fVv9L5ZYcvXD"
   },
   "outputs": [],
   "source": [
    "def plot_image(data, index):\n",
    "    plt.imshow(data.data[index].numpy(), cmap=\"gray\")\n",
    "    plt.title(f\"{data.targets[index]}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(f\"Training data size:\\t {train_data.data.size()}\")  # (60000, 28, 28)\n",
    "print(f\"Training targets size:\\t {train_data.targets.size()}\")  # (60000)\n",
    "\n",
    "plot_image(train_data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zl0EdYfocvXE"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reshape the data into 2D array\n",
    "reshaped_data = train_data.data.numpy().reshape(60000, 28 * 28)\n",
    "\n",
    "# Perform PCA to reduce the data to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(reshaped_data)\n",
    "data_pca = pca.transform(reshaped_data)\n",
    "\n",
    "# Transform the data back to its original size\n",
    "data_pca_inv = pca.inverse_transform(data_pca)\n",
    "\n",
    "# Plot the image in original dimension\n",
    "plt.imshow(data_pca_inv[2].reshape(28, 28), cmap=\"gray\")\n",
    "plt.title(f\"{train_data.targets[2]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "js8De-lkcvXF"
   },
   "source": [
    "**YOUR TURN**\n",
    "* How does the image look after dimensionality reduction compared to the input? ______\n",
    "* Why might it look this way? ______"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkvDQ-ngcvXG"
   },
   "source": [
    "Let's now move on to building an autoencoder for the same task. Autoencoders are easy networks to build, split into the _encoder_, which 'steps' the data down to the final compact representation, and the _decoder_, which is a mirror image of the encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0b71VvTcvXG"
   },
   "outputs": [],
   "source": [
    "myEncoder = nn.Sequential(\n",
    "    nn.Linear(28 * 28, 128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(64, 12),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(12, 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDBfdwsAcvXH"
   },
   "source": [
    "As you may expect, designing the structure of the encoder is something of an art, and it requires balance between the time and input data required to train the network, and performance. Here we are using four step-down operations (the linear layers), which take us from the input of size 784 down to just two dimensions at the bottom. Between each step-down layer is a non-linear activation layer.\n",
    "\n",
    "**YOUR TURN**\n",
    "\n",
    "It would of course be possible to go straight from the input size to the final number of dimensions, but we would lose an incredibly important aspect of neural networks in doing so.\n",
    "\n",
    "* What would we miss out on? ______\n",
    "* Why is this a problem? ______\n",
    "\n",
    "In the cell below, build the structure of the decoder layer for our network. Remember, this is a mirror image of our encoder!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ea1Bqw54cvXH"
   },
   "outputs": [],
   "source": [
    "myDecoder = nn.Sequential(\n",
    "    # YOUR CODE HERE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Ge91UswcvXI"
   },
   "source": [
    "Finally, we just need some boilerplate class code to bring the whole thing together into a PyTorch network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyTHANKjcvXI"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOeLISfQcvXI"
   },
   "source": [
    "Now let's create an instance of our network. We also need to define a few other parameters, like the loss function and the optimizer.\n",
    "\n",
    "For the loss function, we will be using Mean Squared Error, which we covered in the second lab. For the optimizer, let's use an advanced optimizer called Adam:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VedL8oiWcvXJ"
   },
   "outputs": [],
   "source": [
    "autoencoder = AutoEncoder(myEncoder, myDecoder)\n",
    "optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.005)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlozDJtFcvXJ"
   },
   "source": [
    "We'll use a helper function during training which will pass us the data as we go. It's important to remember that for an autoencoder, the input and the label are identical, so we don't have any labels per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssW1oQUQcvXK"
   },
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(dataset=train_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l3BwMYIfcvXK"
   },
   "source": [
    "Our training function is going to show us the recovered images at the end of each epoch, to help us get an idea of how the training process is going. Beforehand, we will just plot out five of the digits so we can compare our autoencoder's output to what the target looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOWHG7AecvXK"
   },
   "outputs": [],
   "source": [
    "def plot_images(view_data, n_images=5):\n",
    "    f, a = plt.subplots(1, n_images, figsize=(5, 2))\n",
    "    for i in range(n_images):\n",
    "        a[i].imshow(np.reshape(view_data.data.numpy()[i], (28, 28)), cmap=\"gray\")\n",
    "        a[i].set_xticks(())\n",
    "        a[i].set_yticks(())\n",
    "\n",
    "\n",
    "N_TEST_IMG = 5\n",
    "view_data = (\n",
    "    train_data.data[:N_TEST_IMG].view(-1, 28 * 28).type(torch.FloatTensor) / 255.0\n",
    ")\n",
    "plot_images(view_data, N_TEST_IMG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4w34AZlcvXL"
   },
   "source": [
    "OK, now we're ready to train! Before running this code, make sure you understand what is happening. The comments should help. Once you feel comfortable, run the cell! It will take a little while to complete, but you will get progress updates as it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1BkZijX_cvXL"
   },
   "outputs": [],
   "source": [
    "def plot_decoded_images(autoencoder, view_data, n_images=5):\n",
    "    # Helper function to plot decoded images\n",
    "    f, a = plt.subplots(1, n_images, figsize=(5, 2))\n",
    "    _, decoded_data = autoencoder(view_data)\n",
    "    for i in range(n_images):\n",
    "        a[i].clear()\n",
    "        a[i].imshow(np.reshape(decoded_data.data.numpy()[i], (28, 28)), cmap=\"gray\")\n",
    "        a[i].set_xticks(())\n",
    "        a[i].set_yticks(())\n",
    "    plt.draw()\n",
    "    plt.pause(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "n_epochs = 25\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for step, (x, _) in enumerate(train_loader):\n",
    "        b_x = x.view(-1, 28 * 28)  # batch x, shape (batch, 28*28)\n",
    "        b_y = x.view(-1, 28 * 28)  # batch y, shape (batch, 28*28)\n",
    "\n",
    "        encoded, decoded = autoencoder(b_x)\n",
    "\n",
    "        loss = loss_func(decoded, b_y)  # calculate error\n",
    "        optimizer.zero_grad()  # reset the gradients, otherwise they will accumulate between epochs\n",
    "        loss.backward()  # compute gradients and backpropagate loss\n",
    "        optimizer.step()  # apply gradients to update our parameters\n",
    "\n",
    "    print(f\"Epoch: {epoch} | train loss: {loss.data.numpy():.4f}\")\n",
    "    plot_decoded_images(autoencoder, view_data, N_TEST_IMG)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaWVZYPOcvXL"
   },
   "source": [
    "The third image in the column is our Autoencoder's recovered version of the number 4 we looked at with PCA. How does it look? Could it be better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYa_8J0zcvXM"
   },
   "source": [
    "We can also plot the encoded data of the autoencoder, to see how it has represented the data in lower 2 dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCIA00n3cvXM"
   },
   "outputs": [],
   "source": [
    "def plot_encoded_data(X, Y, values):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.scatter(\n",
    "        X,\n",
    "        Y,\n",
    "        c=values,\n",
    "        edgecolor=\"none\",\n",
    "        alpha=0.5,\n",
    "        cmap=plt.colormaps.get_cmap(\"nipy_spectral\"),\n",
    "    )\n",
    "    plt.xlabel(\"component 1\")\n",
    "    plt.ylabel(\"component 2\")\n",
    "    plt.colorbar()\n",
    "\n",
    "\n",
    "view_data = train_data.data.view(-1, 28 * 28).type(torch.FloatTensor) / 255.0\n",
    "\n",
    "# Get encoded data\n",
    "encoded_data, _ = autoencoder(view_data)\n",
    "\n",
    "# Get the X, Y dimensions and values\n",
    "X, Y = (\n",
    "    encoded_data.data[:, 0].numpy(),\n",
    "    encoded_data.data[:, 1].numpy(),\n",
    ")  # 2 Dimensions of encoded data\n",
    "values = train_data.targets.numpy()\n",
    "\n",
    "# Plot encoded data\n",
    "plot_encoded_data(X, Y, values)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we are going to plot different outputs based on changes to the representation (encoded) space. We will loop through all different combinations between -1 and 1 to see what the autoencoder has learned."
   ],
   "metadata": {
    "id": "TZYAWeGlorfX"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import Tensor\n",
    "\n",
    "\n",
    "def plot_decoded_image(ax, autoencoder, encoded_vector):\n",
    "    decoded_image = autoencoder.decoder(encoded_vector).detach().numpy().reshape(28, 28)\n",
    "    ax.imshow(decoded_image, cmap=\"gray\")\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "\n",
    "\n",
    "f, a = plt.subplots(9, 9, figsize=(12, 12))\n",
    "for i, v in enumerate(np.linspace(1, -1, 9)):\n",
    "    for j, k in enumerate(np.linspace(-1, 1, 9)):\n",
    "        plot_decoded_image(a[i, j], autoencoder, Tensor([k, v]))"
   ],
   "metadata": {
    "id": "XQVOfMGXoKNd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's utilize the autoencoder's learned embedding space to classify digits. We'll convert input images into embedded features using the encoder of the autoencoder, then pass them to a classifier with their corresponding labels for training. You have the flexibility to choose a classifier such as logistic regression or others. Part of the code is provided below.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Implement a classifier of your choice to train on the input embeddings and labels. You can use a simple classifier such as logistic regression classifier or can try other classifiers of your choice. _________\n",
    "* What is the prediction accuracy of the classifier on the test set? _________"
   ],
   "metadata": {
    "collapsed": false,
    "id": "WVofdEA1XtfE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_data(data, start, end):\n",
    "    \"\"\"Preprocesses the data by reshaping and normalizing it, and returns the labels.\"\"\"\n",
    "    processed_data = (\n",
    "        data.data[start:end].view(-1, 28 * 28).type(torch.FloatTensor) / 255.0\n",
    "    )\n",
    "    labels = data.targets[start:end].numpy()\n",
    "    return processed_data, labels\n",
    "\n",
    "\n",
    "n_samples_train = 100\n",
    "n_samples_test = 10\n",
    "\n",
    "# Preprocess the train and test sets\n",
    "train_set, train_labels = preprocess_data(train_data, 0, n_samples_train)\n",
    "test_set, test_labels = preprocess_data(\n",
    "    train_data, n_samples_train, n_samples_train + n_samples_test\n",
    ")\n",
    "\n",
    "# Obtain embedded representations of train set images using the encoder part of the autoencoder\n",
    "embedded_train = autoencoder.encoder(train_set).detach().numpy()"
   ],
   "metadata": {
    "id": "c5dvfk8TXtfE"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Train your chosen classifier using embedded features and corresponding labels"
   ],
   "metadata": {
    "id": "m7GtXKPvXtfF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Compute the prediction accuracy of the classifier on the test set"
   ],
   "metadata": {
    "id": "hw09W9waXtfF"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
