{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk0LoqhRIhuT"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexwolson/postdocbootcamp2023/blob/main/lab_1_1_neuralnets.ipynb)\n",
    "\n",
    "# UofT DSI-CARTE Postdoc Bootcamp\n",
    "#### Tuesday July 18, 2023\n",
    "#### Intro to Neural Networks in PyTorch - Lab 1, Day 1\n",
    "#### Teaching team: Alex Olson, Nakul Upadhya, Shehnaz Islam\n",
    "##### Lab author: Kyle E. C. Booth, kbooth@mie.utoronto.ca, edited by Jake Mosseri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWWYgttBIhuX"
   },
   "source": [
    "In this lab, we will be taking our first look at developing our own *neural networks* (NN) with [PyTorch](https://pytorch.org/), probably the most popular machine learning library for working with NNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Colab:  False\n"
     ]
    }
   ],
   "source": [
    "# Test whether we are running in Google Colab\n",
    "IN_COLAB = 'google.colab' in str(get_ipython())\n",
    "print('Running in Colab: ', IN_COLAB)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T14:08:41.728364Z",
     "start_time": "2023-07-17T14:08:41.720044Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if not IN_COLAB: # These libraries are already installed in Colab\n",
    "    # Install necessary libraries in one command\n",
    "    !pip install -q torch torchvision torchaudio numpy pandas matplotlib scikit-learn tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-17T14:08:55.931363Z",
     "start_time": "2023-07-17T14:08:52.008829Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LWQyyScGIhuY",
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:02.202552Z",
     "start_time": "2023-07-17T14:08:59.943895Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Cj0g_bBIhuZ"
   },
   "source": [
    "### An Intuitive Intro to Neural Nets\n",
    "\n",
    "In today's lab, we're going to start with an intuitive exercise on the Titanic dataset using Logistic Regression and a simple Neural Network before moving onto some more complex stuff. Let's start by loading our dataset.\n",
    "\n",
    "The Titanic data is stored in a CSV file (located in the 'data' directory of your root folder), so we need to use Pandas to load the data and then separate it into our X (features) and y (target). We also need to: i) drop unimportant columns, and ii) impute missing values.\n",
    "\n",
    "We're going to do this all in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YYb9noojIhua",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:05.289155Z",
     "start_time": "2023-07-17T14:09:03.551098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   pclass  survived  sex      age  sibsp  parch      fare\n0       1         1    0  29.0000      0      0  211.3375\n1       1         1    1   0.9167      1      2  151.5500\n2       1         0    0   2.0000      1      2  151.5500\n3       1         0    1  30.0000      1      2  151.5500\n4       1         0    0  25.0000      1      2  151.5500",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>pclass</th>\n      <th>survived</th>\n      <th>sex</th>\n      <th>age</th>\n      <th>sibsp</th>\n      <th>parch</th>\n      <th>fare</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>29.0000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>211.3375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0.9167</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>30.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>25.0000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>151.5500</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "# Fetch the Titanic dataset as a pandas DataFrame\n",
    "titanic_data = fetch_openml(\"titanic\", version=1, as_frame=True, parser='auto').frame\n",
    "\n",
    "# Convert 'survived' column to numeric values\n",
    "titanic_data['survived'] = pd.to_numeric(titanic_data['survived'])\n",
    "\n",
    "# Remove unnecessary columns\n",
    "columns_to_drop = ['boat', 'body', 'home.dest']\n",
    "titanic_data.drop(columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "# Remove more unimportant columns\n",
    "additional_columns_to_drop = ['name', 'ticket', 'cabin', 'embarked']\n",
    "titanic_data = titanic_data.drop(additional_columns_to_drop, axis=1)\n",
    "\n",
    "# Encode the 'sex' column using LabelEncoder\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "label_encoder.fit(titanic_data['sex'])\n",
    "titanic_data['sex'] = label_encoder.transform(titanic_data['sex'])\n",
    "\n",
    "# Display the first few rows of the processed data\n",
    "titanic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tdHK09KWIhub"
   },
   "source": [
    "Next, we will split the dataset into a training set (where we will do our cross validation) and a test set (our hold-out data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CHYYoq9uIhuc",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:05.373122Z",
     "start_time": "2023-07-17T14:09:05.287396Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split dataset into features and target\n",
    "target_data = titanic_data[\"survived\"]\n",
    "feature_data = titanic_data.drop(\"survived\", axis=1)\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature_data, target_data, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4UvxOkuoIhud"
   },
   "source": [
    "Now, we're ready to try out some models on our training data. Since we're solving a binary classification problem (i.e., predicting a 0 or 1 target), we want to design classifiers. Examples of simple machine learning classifiers include: **k-nearest neighbors**, **decision trees**, and **logistic regression**.\n",
    "\n",
    "In this exercise, we're going to fit a logistic regression to our data and then design a neural network architecture that behaves exactly like a logistic regression and validate that we get the same result.\n",
    "\n",
    "##### Intro: Logistic Regression\n",
    "\n",
    "Logistic regression models are linear models similar to linear regression models. The linear regression equation can be written as follows:\n",
    "\n",
    "\n",
    "<center>$\\hat{y} = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n$</center>\n",
    "\n",
    "Where $\\hat{y}$ is our prediction, $\\beta$ is our vector of coefficients (the things we learn), and $x$ is our feature vector. The linear regression equation defines a line in $n$ dimensional space. The problem with linear regression is that it doesn't really perform well on classification tasks. Consider the following example:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/linear-classification.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "The green line represents our trained linear regression model. Our feature is the size of a tumor, and our target is whether it is malignant or not (0 or 1). As we can see, even though our model is trained to the data to minimize error, for a lot of the values of tumor size it is going to give us a weird result (e.g., for some really small tumors, the prediction would be a negative value!).\n",
    "\n",
    "To resolve this, we use the *logistic function* (also called the *sigmoid* function) to 'squish' our linear model to be bounded by 0 and 1. The logistic (sigmoid) function is $\\frac{1}{1+e^{-x}}$, and thus our logistic regression equation becomes:\n",
    "\n",
    "<center>$\\large\\hat{y} = \\frac{1}{1+e^{-(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 +, \\dots, + \\beta_n x_n)}}$</center>\n",
    "\n",
    "In this equation, the values for $\\hat{y}$ can never be below 0, and can never exceed 1 even for the most extreme feature values $x$.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* Assuming you've trained a nice logistic regression model to the below data (see Figure), what might the model fit look like (i.e., what will the line look like)? ____________________________________\n",
    "* For new data samples with features $x$, how would you convert the output of the logistic regression, $\\hat{y}$, into a classification (0 or 1)? ______________________________\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/logistic-classification.jpg?raw=1\" width=\"400\"/>\n",
    "\n",
    "OK, cool! Let's use scikit-learn to fit a logistic regression model to our training set and then predict on our test set (we won't do cross validation this time).\n",
    "\n",
    "Before we can fit our logistic regression model to our training data, we can conduct imputation to replace missing values with the mean/median/mode value in the column. For this exercise we will conduct mode imputation (i.e., the most common value in the column).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "t7iInLxCIhue",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:08.221099Z",
     "start_time": "2023-07-17T14:09:08.027997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic accuracy: 79.13%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Impute missing values using the most frequent strategy\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "imp.fit(X_train)\n",
    "\n",
    "# Transform both the training and test sets\n",
    "X_train = imp.transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "# Fit the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = logreg.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Logistic accuracy: {accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bX9iX-X4Ihuf"
   },
   "source": [
    "The results show our logistic regression is 79.13% accurate in its predictions.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* The default regularization parameter for sklearn's logistic regression is L2 (or ridge regression); can you figure out how to change it to L1 (LASSO)? _Note: you'll have to look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)!_ ______________________\n",
    "* What is the mean accuracy of an L1 regularized Logistic regression model on the training set? ______________________\n",
    "\n",
    "OK - time for the good stuff!\n",
    "\n",
    "### Neural Networks\n",
    "\n",
    "A *neural network* (NN) is a type of machine learning model that, like linear or logistic regression, takes a feature vector, X, as input and predicts a target, y. The way it does this is a little bit different, however. A typical NN architecture consists of: an input layer, hidden layers, and an output layer. Each layer consists of a set of nodes (neurons) connected by edges (outputs). Let's look at the figure below:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/nn.jpeg?raw=1\" width=\"400\"/>\n",
    "\n",
    "**Input layer**: This is a passive layer that simply takes in your feature data and outputs it to the hidden layers. You can think of each input layer neuron as being associated with a feature in your feature set.\n",
    "\n",
    "**Hidden layer**: This is where the magic happens. The original features, as received by the input layer, go through a series of transformations within the hidden layer. You can think of each node (neuron) within the hidden layer as a highly transformed feature.\n",
    "\n",
    "**Output layer**: This is where we get our final result, the 0 or 1 prediction.\n",
    "\n",
    "### Zooming In\n",
    "\n",
    "Let's take a look at what is happening at any given node (neuron) within the hidden layer. Take a look at the following image of a neuron within an NN:\n",
    "\n",
    "<img src=\"https://github.com/lyeskhalil/mlbootcamp/blob/master/img/neuron.png?raw=1\" width=\"500\"/>\n",
    "\n",
    "Every neuron has some inputs ($x_1, x_2, \\dots, x_n$) with input weights ($w_1, w_2, \\dots, w_n$) and an output, $Y$. The neuron itself applies a transformation, $f$, known as the *activation function*, to the linear combination of its inputs and input weights. The value $b$ is a constant weight called the bias.\n",
    "\n",
    "There are many different types of activation functions, but the popular ones are the *sigmoid*, *tanh*, and *ReLU* activation functions. Yes, you heard correctly: the sigmoid function is a popular activation function! (This should be reminding you of the logistic regression model we discussed above).\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If you were to develop a simple neural network architecture that was equivalent to a logistic regression model for the Titanic data, how would you do it? Get a pen and paper and draw it out. Make sure to specify: the input layer, the hidden layer(s), the output layer, the activation function(s), the weights, and the biases.\n",
    "* How many hidden layers does your NN have? What type of activation function, $f$, does it use? _________________\n",
    "* Say you wanted to add another layer to your NN architecture with 3x neurons, what would your new architecture look like? _________________\n",
    "\n",
    "### Intro to  PyTorch\n",
    "\n",
    "OK, so now that we've made the connection between NNs and Logistic Regression, let's code up our little NN in PyTorch and use it to predict survivorship on the Titanic dataset.\n",
    "\n",
    "First, *tensors* are the fundamental data type of PyTorch. Each tensor is effectively a multi-dimensional array, just like a numpy array. The primary difference is that tensors have been setup in such a way to enhance the NN training process.\n",
    "\n",
    "Let's load our X and y training data into tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gxobcMqFIhug",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:15.688537Z",
     "start_time": "2023-07-17T14:09:11.984324Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train).float()\n",
    "y_train_tensor = torch.tensor(y_train.values).float()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test).float()\n",
    "y_test_tensor = torch.tensor(y_test.values).float()"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"Shape of X_train_tensor:\", X_train_tensor.shape)\n",
    "print(\"Shape of y_train_tensor:\", y_train_tensor.shape)\n",
    "print(\"Shape of X_test_tensor:\", X_test_tensor.shape)\n",
    "print(\"Shape of y_test_tensor:\", y_test_tensor.shape)"
   ],
   "metadata": {
    "id": "Ge4UKjvvuNy3",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:15.694595Z",
     "start_time": "2023-07-17T14:09:15.690575Z"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_tensor: torch.Size([916, 6])\n",
      "Shape of y_train_tensor: torch.Size([916])\n",
      "Shape of X_test_tensor: torch.Size([393, 6])\n",
      "Shape of y_test_tensor: torch.Size([393])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCDsVueiIhuh"
   },
   "source": [
    "Next, we will actually define our logistic regression network model class. The below function, `LogisticRegression`, applies a sigmoid transformation to the output, as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vnbPW--OIhuh",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:16.963247Z",
     "start_time": "2023-07-17T14:09:16.956499Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = torch.sigmoid(self.linear(x))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OBNTYMbRIhuh"
   },
   "source": [
    "Next, we identify the dimensions of our problem: 6 x 1 (6 features and 2 target classes: 0 or 1), initialize our model with those dimensions and then specify the loss function ([cross entropy](https://en.wikipedia.org/wiki/Cross_entropy)) and optimization technique ([stochastic gradient descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "zAfHj63fIhui",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:17.976895Z",
     "start_time": "2023-07-17T14:09:17.965262Z"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = 6\n",
    "output_dim = 1\n",
    "\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "LogisticRegression(\n  (linear): Linear(in_features=6, out_features=1, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "id": "4-HWmInkYMAZ",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:18.381329Z",
     "start_time": "2023-07-17T14:09:18.369930Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyFpvFy_Ihui"
   },
   "source": [
    "Next, we set up our dataset to be *iterable* such that we can train our neural network in *batches*. A batch is a subset of the total data such that if we combined them all, we'd get the whole dataset. Batching is done to speed up the training process and reduce memory requirements.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If we select a batch size of 256, how many batches of training data will be generated?________________\n",
    "\n",
    "The `DataLoader` function does this batching operation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "yTtyV_mVIhuj",
    "ExecuteTime": {
     "end_time": "2023-07-17T14:09:20.121970Z",
     "start_time": "2023-07-17T14:09:20.115115Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ltyMFvsIhuj"
   },
   "source": [
    "Next, we train our NN and test its performance on the test set every 4000 iterations. We use 40,000 *epochs* and print our accuracy values every 4000 iterations. An epoch is when the entire dataset has passed through the network. An iteration is when a single forward/backward pass of the network over a batch of data is done.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* If our batch size is 256 and we elect to do 40,000 epochs (i.e., the network sees the entire dataset 40,000 times), how many iterations (forward/backward passes of the data) will our network see? _______________________\n",
    "\n",
    "Run the below code and check out the incremental accuracy improvement output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def report_progress(training_model, x, y, tb_writer, current_iter):\n",
    "    training_model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.round(training_model(x).data).numpy()\n",
    "    training_model.train()\n",
    "\n",
    "    train_accuracy = accuracy_score(y, prediction)\n",
    "    train_progress = epoch / epochs * 100\n",
    "\n",
    "    # Write loss and accuracy to TensorBoard\n",
    "    tb_writer.add_scalar(\"Loss\", loss.item(), current_iter)\n",
    "    tb_writer.add_scalar(\"Accuracy\", train_accuracy, current_iter)\n",
    "\n",
    "    print(f\"Loss: {loss.item():.2f} | Accuracy: {train_accuracy:.2f} | Progress: {train_progress:.2f}% | Iteration: {current_iter}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2lxP28lIhuj"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "iterations = 0\n",
    "epochs = 40000\n",
    "\n",
    "# Create a SummaryWriter to write TensorBoard logs\n",
    "writer = SummaryWriter(log_dir='runs', comment='Titanic_Logistic_Regression')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (features, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()                            # Reset the gradients so they are not accumulated between batches\n",
    "        outputs = model(features)                        # Get the model's predictions\n",
    "        loss = criterion(torch.squeeze(outputs), target) # Calculate the error in the model's predictions\n",
    "        loss.backward()                                  # Calculate the gradients using backpropagation\n",
    "        optimizer.step()                                 # Update the weights\n",
    "\n",
    "        iterations += 1\n",
    "        if iterations % 4000 == 0:\n",
    "            report_progress(model, X_train_tensor, y_train_tensor, writer, iterations)\n",
    "\n",
    "# Close the SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Start TensorBoard\n",
    "%tensorboard --logdir runs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YuXmjwuMIhuk"
   },
   "source": [
    "Hopefully you got an accuracy of around ~78%. What you'll notice is that is similar accuracy we got from sklearn's built-in logistic regression function from earlier in the lab. There are differences in the implementation that account for this discrepancy, notably sklearn used l2 regularization, and an optimizer called LBFGS instead of SGD that we learned and used.\n",
    "\n",
    "Let's take a look at the trained model parameters using the `model.parameters()` function within PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "535umxhZIhuk"
   },
   "outputs": [],
   "source": [
    "params = list(model.parameters())\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gkfrWNPGIhuk"
   },
   "source": [
    "Hm, well this is interesting! We can see that our model consists of two tensors: the first has (1,6), and the second has dimension (1). Refer back to how you drew what you thought this NN architecture would look like.\n",
    "\n",
    "**YOU TURN:**\n",
    "* What do you think these values represent? ____________________________\n",
    "* How many hidden layers does the architecture have? ______________________________\n",
    "* Draw the architecture and label (some of) the weights (trained parameters). ______________________________\n",
    "\n",
    "(*Hint: to answer these questions, try printing `outputs.data[0]` and `predicted[0]` to look at the model's assessment of the first sample*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YeZvj7ekIhul"
   },
   "source": [
    "You'll notice that this took considerably longer to train than scikit-learn's logistic regression: this is because PyTorch is set-up to be more flexible and train architectures much more complex than a simple single neuron network. Scikit-learn's implementation of logistic regression is highly optimized.\n",
    "\n",
    "**YOUR TURN:**\n",
    "* How many epochs would you need to increase the process to 200,000 iterations? ______________________\n",
    "* Does increasing to 200,000 iterations improve your test set accuracy? ______________________\n",
    "* Compare the predictions of your logistic regression from scikit-learn and your network developed with PyTorch. Are all the predictions the same? How many predictions are pair-wise different? ______________________\n",
    "\n",
    "* Create a simple three layer fully connected neural network with a single hidden layer of 4 neurons followed by Relu activation, and learning rate of 0.01. What is the final accuracy of this network? ______________________\n",
    "<br>(Hint: If you are using **BCELoss** as loss criterion, you need to use a **sigmoid** activation function for the output layer. If you are using **CrossEntropyLoss** as loss criterion, you need to use a **softmax** activation function for the output layer. While BCELoss is used for binary classification, CrossEntropyLoss is used for multi-class classification.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UEscia6QIhul"
   },
   "source": [
    "Congratulations! You've completed an introduction to neural networks and PyTorch. If you want to explore more sophisicated architectures and applications, check out designing a PyTorch neural network to properly classify digit images here:\n",
    "\n",
    "https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627\n",
    "\n",
    "Other than that, you're done the lab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
