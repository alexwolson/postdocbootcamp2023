{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06zurkFMFSzK"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/alexwolson/postdocbootcamp2023/blob/main/lab_3_2_interpretability.ipynb)\n",
    "\n",
    "# UofT DSI-CARTE ML Bootcamp\n",
    "#### July 30th, 2023\n",
    "#### Interpretability - Lab 2, Day 3\n",
    "#### Teaching team: Alex Olson, Nakul Upadhya, Shehnaz Islam\n",
    "##### Lab author: Nakul Upadhya\n",
    "\n",
    "Machine learning models have revolutionized numerous fields by delivering remarkable predictive capabilities. However, as these models become increasingly ubiquitous, the need to ensure fairness and interpretability has emerged as a critical concern. In this lab, we will show how you can analyze and interpret your models.\n",
    "\n",
    "\n",
    "The main packages we will be using in this lab is `shap` and `captum` along with all the other packages we have previously used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61U22Ng1FLqe"
   },
   "outputs": [],
   "source": [
    "!pip install shap\n",
    "!pip install captum\n",
    "!pip install torch torchvision torchaudio\n",
    "!pip install tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJOgyKivHD6p"
   },
   "source": [
    "## Data\n",
    "For this analysis, we will be working with the COMPAS dataset, a dataset used by the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) system. This is a popular tool used in the United States to assess the risk of recidivism (likelihood of re-offending) for individuals involved in the criminal justice system. ***The system is currently actively employed by judges and parole boards to make decisions about bail, sentencing, and parole.***\n",
    "\n",
    "The COMPAS system has been criticized for exhibiting racial and gender biases. Many studies have suggested that the tool is more likely to label Black defendants as high-risk and White defendants as low-risk, even when controlling for other factors. This has raised concerns about fairness and potential discrimination in decision-making processes. For more information about COMPAS, I highly encourage reading the [Propublica article that broke the story](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing).\n",
    "\n",
    "This highly controversial system and the data it uses perfectly demonstrates the need for interpretability in machine learning. Lets start by reading and preprocessing the data. Some of the code for preprocessing has been borrowed from [this notebook.](https://github.com/tsotne95/FairnessCompas/blob/master/Fairness_in_Classification_on_the_COMPAS_dataset.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vMLnBLcHGkbV"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/tsotne95/FairnessCompas/master/compas-scores-two-years.csv\")\n",
    "print(\"Original Entries in dataset\")\n",
    "print(df.shape)\n",
    "\n",
    "df = df.dropna(subset=[\"days_b_screening_arrest\"]) # dropping missing vals\n",
    "df = df[(df.days_b_screening_arrest <= 30) & (df.days_b_screening_arrest >= -30) & (df.is_recid != -1) & (df.c_charge_degree != 'O') & (df.score_text != 'N/A') ]\n",
    "df.reset_index(inplace=True, drop=True) # renumber the rows from 0 again\n",
    "\n",
    "## keeping relevant columns\n",
    "\n",
    "df = df[['sex','age','race','juv_fel_count','juv_misd_count','juv_other_count',\n",
    "         'priors_count','two_year_recid', 'c_charge_degree']]\n",
    "df['sex'] = df['sex'].replace({\n",
    "    'Male': 1,\n",
    "    'Female': 0\n",
    "})\n",
    "df.rename({'c_charge_degree':'felony'}, axis=1, inplace=True)\n",
    "df['felony'] = df['felony'].replace({\n",
    "    \"F\": 1,\n",
    "    \"M\": 0\n",
    "})\n",
    "race_df = pd.get_dummies(df[['race']])\n",
    "race_df.columns = [x[5:] for x in race_df.columns]\n",
    "no_race_df = df.drop(columns=['race'])\n",
    "df = no_race_df.join(race_df)\n",
    "df = df.dropna()\n",
    "print(\"Entries in dataset after preprocessing\")\n",
    "print(df.shape)\n",
    "\n",
    "X = df.drop(columns = ['two_year_recid'])\n",
    "y = df['two_year_recid']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Print information about the final dataset\n",
    "print(f\"There are {X_train.shape[0]} training data points and {X_test.shape[0]} testing points\")\n",
    "print(f\"There are {X_train.shape[1]} features in the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDEXNF7yLJ4L"
   },
   "source": [
    "This data has already been pre-processed for us, so lets simply split it up into train and test datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5icRqVnMJHaM"
   },
   "source": [
    "## Inherent Interpretability\n",
    "When dealing with sensitive information, it is almost always recommended to use *inherently interpretable* (also known as glassbox) models. These are models whos decision making mechanisms are exposed to the user, therefore any systemic issues present in the model can be found much more easily. The most common glassbox models are Decision Trees and Linear/Logistic Regression.\n",
    "\n",
    "With Logistic Regression, the weights assigned to each feature can be easily visuaized, allowing users to understand which features the model found important which features are less significant.\n",
    "\n",
    "Decision trees also have a similar benefit as one can visualize the tree and simply read the decision splits in each node. This allows for a similar sense of feature importance. One downside of decision trees however is that this glassbox nature significantly goes down as you increase the tree depth.\n",
    "\n",
    "Both of these models have both local interpretability (it is easy to explain why a single prediction was made) and global interpretability (one can explain patterns in decision mechanisms for all the points).\n",
    "\n",
    "**Your Turn**\n",
    "1. In the cell below, change the max_depth of the tree to a depth you think would be interpretable to a majority of stakeholders. Use the tree visualization code provided to gauge your own ability to interpret the model.\n",
    "2. Looking at the generated tree, are there any significant issues or problems you notice with the prediction mechanisms?\n",
    "3. If we were to deploy the trained model below, what potential ethical and legal concerns do you think we need to consider?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvVOkQ5NLtKy"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth = 2) # CHANGE TO YOUR OWN VALUE\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate and print testing accuracy\n",
    "test_predictions = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "print(f\"Testing Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQarlbUoM8Xq"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "plot_tree(clf,\n",
    "          feature_names = X_test.columns,\n",
    "          filled = True,\n",
    "          ax = ax,\n",
    "          impurity = False,\n",
    "          class_names = [\"False\",\"True\"],\n",
    "          fontsize = 6\n",
    "          );\n",
    "## This tree is shaded based on the number of data points that fall into that node\n",
    "## and the label of those datapoints. Ex. A dark blue node means that most of\n",
    "## training datapoints that fall into that node did commit a crime within\n",
    "## two years of release. Dark orange is the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "79htGMp6JH1X"
   },
   "source": [
    "## Post-Hoc Interpretability\n",
    "In an ideal world, all problems could be solved via interpretable models. Unfortunately, these models are often sparse and may not have a significant amount of predictive power. In these cases, we may want to use *black-box* models whose internal works are unknown to the users and instead try and *estimate* what features are being used by the model. These explanations are called *post-hoc* explanations.\n",
    "\n",
    "Before that, lets do one more pre-processing step. All the models following this will perform better when scaled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6h4q_0elZ1oc"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])\n",
    "# X_test[X_test.columns] = scaler.transform(X_test[X_test.columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kjAqhUTuZ11h"
   },
   "source": [
    "\n",
    "### SHAP\n",
    "One post-hoc interpretability tool is SHAP (Shapely Additive Values). SHAP provides feature importance by using methods from game theory to estimate the contribution of each feature towards the final prediction.\n",
    "\n",
    "SHAP can provide a sense of both local (explaining a single prediction) and global (explaining general prediction trends) interpretability.\n",
    "\n",
    "To start, we first need to train a model to explain. For this exercise, we will use a random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nvGC0g0XJH6_"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(max_depth = 2)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "accuracy = accuracy_score(y_test, rf_clf.predict(X_test)) * 100\n",
    "print(f\"Test Accuracy: {accuracy :.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeHvdMrdbbCn"
   },
   "source": [
    "Now to start our explanation process. We start off by first creating a summary of our dataset (this is to make SHAP run faster) and creating our explainer object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3-57iPcbaZC"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "# this helps everything run faster\n",
    "X_train_summary = shap.kmeans(X_train, 7)\n",
    "\n",
    "# Create the shap explainer by passing in our model's predict function and\n",
    "# the summarized training set\n",
    "ex = shap.KernelExplainer(rf_clf.predict, X_train_summary, feature_names = X_train.columns)\n",
    "\n",
    "# We are also only going to look at 100 points (to make it easier to visualize)\n",
    "X_test_subset = X_test.sample(100, random_state = 42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcOmdzntbh1-"
   },
   "source": [
    "#### Local Explainability\n",
    "Let's first look into the local explainability provided by SHAP by examining what contributes to the predictions of the first datapoint in our testing subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "if5xc4e6bhTH"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "first_datapoint = X_test_subset.iloc[0]\n",
    "single_point_shap_values = ex.shap_values(first_datapoint)\n",
    "shap.force_plot(ex.expected_value, single_point_shap_values, X_test_subset.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLFwvs-Cbmye"
   },
   "source": [
    "In the plot above, feature values that increased the chance of the model predicting a repeat crime are in red and have arrows that point to the right (they provide a positive force) and feature values that detract from the probability of recidivism are in blue and point to the left. The larger the arrow, the larger the contribution.\n",
    "\n",
    "**Your Turn**\n",
    "\n",
    "* What features seem to have the most negative impact to the end prediction of the data point you chose? What about the one with the most positive impact? *YOUR ANSWER HERE*\n",
    "* Choose a different data point and see if you see any similarities in the features used and their impact towards the end prediction. *YOUR ANSWER HERE*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9AMlQo4b2is"
   },
   "source": [
    "#### Global Explanability\n",
    "We can get a sense of global interpretability from SHAP by examining trends in the SHAP values across the variable values. To do this, we can generate a summary plot of the calculated values. NOTE: This may take a while....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wAkekm0LblXb"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap_values = ex.shap_values(X_test_subset)\n",
    "shap.summary_plot(shap_values, X_test_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgMWVW2Bb5kX"
   },
   "source": [
    "The color of the point reflects the value of a given feature in a given data point. For example, a red point in `priors_count` means that the feature took a value of 1 (true) and a blue point means a value of 0 (false). The X-axis of this plot represents SHAP contribution (the estimated impact on the end model prediction). By examining the distribution of the feature values across the x-axis, we can find what features may heavily impact the end prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Qui9HVzb4wC"
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "abs_shap_values = np.abs(shap_values)\n",
    "shap_sums = np.sum(abs_shap_values, axis=0)\n",
    "importances = pd.DataFrame()\n",
    "importances['feature'] = X_train.columns\n",
    "importances['importance'] = shap_sums\n",
    "importances.sort_values(by = 'importance', inplace =True, ascending = False)\n",
    "importances = importances[importances['importance'] > 0]\n",
    "px.bar(importances, x = 'feature', y = 'importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bgk2FBxNcR7y"
   },
   "source": [
    "**Your Turn**\n",
    "\n",
    "* What features seem important to the XGBoost model according to SHAP? *YOUR ANSWER HERE*\n",
    "* Do these SHAP importances align with the Decision Tree Importances? *YOUR ANSWER HERE*\n",
    "* Do you see any concerning trends?\n",
    "\n",
    "#### Notes about SHAP\n",
    "SHAP is an incredibly powerful tool to understand what your model may be doing, ***however it is only an estimate***. The SHAP value calculations only examine your model's behavior and do not dive into the internals of the model, therefore these values should not be taken at face value. Additionally, as you may have noticed in the plots above, SHAP values do not reflect interacting effects between features, something that most models do in fact use. This extends to other post-hoc interpretability methods as well.\n",
    "\n",
    "As such, it is highly encouraged to use innately interpretable models whenever possible. For a more rigourous justification, please read Cynthia Rudin's paper on the subject after the lab.\n",
    "\n",
    "Additionally, quoting the SHAP documentation:\n",
    "\n",
    "\n",
    "> Predictive machine learning models like XGBoost become even more powerful when paired with interpretability tools like SHAP. These tools identify the most informative relationships between the input features and the predicted outcome, which is useful for explaining what the model is doing, getting stakeholder buy-in, and diagnosing potential problems. It is tempting to take this analysis one step further and assume that interpretation tools can also identify what features decision makers should manipulate if they want to change outcomes in the future. However, in [this article](https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%C2%A0insights.html), we discuss how using predictive models to guide this kind of policy choice can often be misleading.\n",
    "\n",
    "> *Eleanor Dillon, Jacob LaRiviere, Scott Lundberg, Jonathan Roth, and Vasilis Syrgkanis from Microsoft.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7nLC4-AJIOx"
   },
   "source": [
    "### Captum for Neural Network Interpretability\n",
    "\n",
    "Post-Hoc explanations can also be generated for Neural Networks. One common method for generating these is through leveraging the gradients present in the model and is aptly named Input x Gradient. By multiplying the input by the gradients of the neurons in the network, we can get a sense of the importance the network attributes to the input features. The `captum` package provides us with this functionality.\n",
    "\n",
    "\n",
    "#### MLP Network\n",
    "First lets create and train a basic classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbAVYVxwJIj_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm.notebook import trange\n",
    "class CompasMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    three-layer network for the COMPAS dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, batch_size, learning_rate, epochs):\n",
    "        super(CompasMLP, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(num_features = input_dim) # scales input data\n",
    "\n",
    "        # Define the forward pass layers\n",
    "        self.forward_pass = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass.\n",
    "        \"\"\"\n",
    "        return self.forward_pass(self.bn(x))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the model.\n",
    "        \"\"\"\n",
    "        self.train()\n",
    "\n",
    "        # Create tensors\n",
    "        X_tensor = torch.Tensor(X.values).float()\n",
    "        Y_tensor = torch.Tensor(y.values).float()\n",
    "\n",
    "        # Create DataLoader\n",
    "        train_dataset = TensorDataset(X_tensor, Y_tensor)\n",
    "        train_loader = DataLoader(dataset=train_dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        # Define loss function and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        pbar = trange(self.epochs)\n",
    "        # Training loop\n",
    "        for epoch in pbar:\n",
    "          losses = []\n",
    "          for batch_idx, (features, target) in enumerate(train_loader):\n",
    "              optimizer.zero_grad()  # reset gradients\n",
    "              outputs = self.forward(features)  # forward pass\n",
    "              loss = criterion(torch.squeeze(outputs), torch.squeeze(target))  # calculate loss\n",
    "              loss.backward()  # backpropagation\n",
    "              optimizer.step()  # update weights\n",
    "              losses.append(loss.item())\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the class of the input data X.\n",
    "        \"\"\"\n",
    "        self.eval()  # switch to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.Tensor(X.values)\n",
    "            y_pred = torch.sigmoid(self.forward(X_tensor))  # apply sigmoid for binary output\n",
    "            y_pred = torch.round(y_pred).squeeze().numpy()  # round to nearest integer (0 or 1) and convert to numpy array\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfX95ID0c_lB"
   },
   "outputs": [],
   "source": [
    "mlp_clf = CompasMLP(X_train.shape[1],\n",
    "                    64,\n",
    "                    512,\n",
    "                    1e-3,\n",
    "                    1000)\n",
    "mlp_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = mlp_clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Testing Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7H723Bxnj_Wp"
   },
   "source": [
    "#### Using Captum\n",
    "So now lets calculate the model explanation for the first datapoint in the set.\n",
    "We can simply call the InputXGradient explainer on any datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1689560763158,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "JJxBXmxNj-sp",
    "outputId": "b5e844f0-242a-4608-fb68-0c18962c2efd"
   },
   "outputs": [],
   "source": [
    "from captum.attr import InputXGradient\n",
    "\n",
    "mlp_clf.eval()\n",
    "\n",
    "# Pass our model into the explainer\n",
    "input_x_grad = InputXGradient(mlp_clf)\n",
    "\n",
    "# get the first datapoint\n",
    "datapoint = X_test_subset.iloc[0]\n",
    "input = torch.Tensor(datapoint.values).unsqueeze(0)\n",
    "input.requires_grad = True\n",
    "\n",
    "# Get our prediction\n",
    "prediction = mlp_clf(input)\n",
    "\n",
    "# Get the attribution\n",
    "attribution = input_x_grad.attribute(input).detach().numpy()[0]\n",
    "print(f\"Prediction: {np.round(torch.sigmoid(prediction).item())}\")\n",
    "for i in range(len(datapoint)):\n",
    "  print(f\"{datapoint.index[i]} = {datapoint[i]} ===> {attribution[i]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHBffiitj-_M"
   },
   "source": [
    "In this explanation, a negative value means that the value of the datapoint decreased the models prediction and made it less likely to predict recidivism. A positive value is the opposite.\n",
    "\n",
    "**Your Turn**\n",
    "1. For the datapoint, which features were used and did the features align with the SHAP values for the same point above?\n",
    "\n",
    "\n",
    "Input x Gradient provides local interpretability. If we want to get a sense of global interpretability and global feature importance, we can pass in multiple values at once and take the absolute sum of the columns.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 559
    },
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1689560895986,
     "user": {
      "displayName": "Nakul Upadhya",
      "userId": "08924826005411940959"
     },
     "user_tz": 240
    },
    "id": "P9fjBUw-pwT-",
    "outputId": "787addaf-c3d7-4dce-ff6c-2410451c190e"
   },
   "outputs": [],
   "source": [
    "datapoints = X_test_subset\n",
    "input = torch.Tensor(datapoints.values)\n",
    "input.requires_grad = True\n",
    "\n",
    "attribution = input_x_grad.attribute(input).detach().numpy()\n",
    "average_attr = np.sum(np.abs(attribution), axis =0)\n",
    "\n",
    "importances = pd.DataFrame()\n",
    "importances['feature'] = X_test_subset.columns\n",
    "importances['importance'] = average_attr\n",
    "importances.sort_values(by = 'importance', inplace =True, ascending = False)\n",
    "importances = importances[importances['importance'] > 0]\n",
    "px.bar(importances, x = 'feature', y = 'importance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQBk1_OGpyif"
   },
   "source": [
    "**Your Turn**\n",
    "1. Now that we have explained all the models. Which model and explanation do you trust the most? How did you decide this?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
